import argparse
import re
import sys
import requests
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ============ åŸºæœ¬é…ç½® ============
LOCAL_MODEL_PATH = r"C:/Users/nashk/Documents/nashknight/search_agent/Qwen3-0.6B"

# é•¿åº¦æ§åˆ¶å‚æ•°ï¼ˆä¸ notebook ä¿æŒä¸€è‡´ï¼‰
MAX_SOURCES_PER_SEARCH = 5
MAX_SOURCE_DESC_LEN = 400
MAX_FORMATTED_SOURCES_LEN = 1500
MAX_RAW_DISPLAY_LEN = 1000

BASE_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·é—®é¢˜,åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢æ¥è·å–ä¿¡æ¯ã€‚\n\n"
    "**æ ¸å¿ƒåˆ¤æ–­åŸåˆ™:**\n"
    "åœ¨åˆ†æé—®é¢˜æ—¶,å¦‚æœé‡åˆ°ä»¥ä¸‹æƒ…å†µ,å¿…é¡»ä½¿ç”¨<search>æ ‡ç­¾æœç´¢:\n"
    "1. éœ€è¦å®æ—¶æ•°æ®(è‚¡ä»·ã€å¤©æ°”ã€æ–°é—»ã€æ±‡ç‡ç­‰ä¼šå˜åŒ–çš„ä¿¡æ¯)\n"
    "2. éœ€è¦æœ€æ–°ä¿¡æ¯(å½“å‰çŠ¶æ€ã€ä»Šå¤©/ç°åœ¨çš„æƒ…å†µ)\n"
    "3. éœ€è¦å…·ä½“äº‹å®(æŸä¸ªå…¬å¸çš„æ•°æ®ã€æŸä¸ªåœ°ç‚¹çš„æƒ…å†µã€æŸä¸ªäº§å“çš„å‚æ•°)\n"
    "4. ä½ ä¸ç¡®å®šç­”æ¡ˆ,æˆ–ç­”æ¡ˆå¯èƒ½è¿‡æ—¶\n"
    "5. ç”¨æˆ·æ˜ç¡®è¦æ±‚æŸ¥è¯¢ã€æœç´¢ã€æŸ¥æ‰¾ä¿¡æ¯\n\n"
    "å¦‚æœæ˜¯ä»¥ä¸‹æƒ…å†µ,å¯ä»¥ç›´æ¥å›ç­”:\n"
    "1. å¸¸è¯†æ€§é—®é¢˜(å¦‚ä½•åšæŸäº‹ã€æ¦‚å¿µè§£é‡Š)\n"
    "2. ä¸»è§‚é—®é¢˜(å»ºè®®ã€æ„è§)\n"
    "3. æ•°å­¦è®¡ç®—ã€é€»è¾‘æ¨ç†\n\n"
    "**ç¡®å®šéœ€è¦æœç´¢æ—¶çš„è¾“å‡ºæ ¼å¼**:\n"
    "<search>ç®€çŸ­æŸ¥è¯¢è¯</search>\n\n"
    "**ä¸éœ€è¦æœç´¢æ—¶,ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚**\n\n"
    "**å…³é”®è§„åˆ™:**\n"
    "- <search>æ ‡ç­¾å•ç‹¬æˆè¡Œ,å†…å®¹3-10å­—\n"
    "- æ²¡æœç´¢å‰ä¸è¦ç¼–é€ æ•°å­—ã€æ—¥æœŸç­‰äº‹å®\n"
    "- å®å¯å¤šæœç´¢,ä¸è¦çŒœæµ‹\n"
    "- å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯,ç›´æ¥å›ç­”å¹¶åˆ—å‡ºå‚è€ƒæ¥æº,ä¸è¦å†è¾“å‡º<search>\n"
)

# Jina Search API é…ç½®ï¼ˆä¿æŒä¸ notebook ä¸€è‡´ï¼Œå¯æŒ‰éœ€ä¿®æ”¹ä¸ºç¯å¢ƒå˜é‡ï¼‰
JINA_API_KEY = "jina_800f62ec9cc745e09f058c4652a961feziG6FeCa71toa9my7gXm3prQbJaF"
JINA_SEARCH_ENDPOINT = "https://s.jina.ai"
PROXIES = {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"}

# ============ æ ¸å¿ƒæ¨¡å‹ä¸å·¥å…·å‡½æ•°ï¼ˆä» notebook ç²¾ç®€ç§»æ¤ï¼‰ ============

def load():
    tok = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, local_files_only=True)
    m = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH,
        local_files_only=True,
        device_map="auto" if torch.cuda.is_available() else None,
    )
    return m, tok

model, tokenizer = load()


def generate(prompt, max_new_tokens=512):
    msgs = [{"role": "user", "content": prompt}]
    if hasattr(tokenizer, "apply_chat_template"):
        text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    else:
        text = prompt
    inp = tokenizer(text, return_tensors="pt")
    for k, v in inp.items():
        inp[k] = v.to(model.device)
    ids = model.generate(**inp, max_new_tokens=max_new_tokens)
    gen_ids = ids[0][inp["input_ids"].shape[-1]:]
    raw_text = tokenizer.decode(gen_ids, skip_special_tokens=False)
    clean_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return gen_ids.tolist(), raw_text, clean_text


def clear_model_cache():
    if hasattr(model, "past_key_values"):
        model.past_key_values = None
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def fetch_search_result(query: str, use_proxy: bool = True, max_sources: int = MAX_SOURCES_PER_SEARCH) -> dict:
    headers = {
        "Accept": "application/json",
        "Authorization": f"Bearer {JINA_API_KEY}",
        "X-Respond-With": "no-content",
    }
    url = f"{JINA_SEARCH_ENDPOINT}/?q={requests.utils.quote(query)}"
    proxies_cfg = PROXIES if use_proxy else None
    try:
        resp = requests.get(url, headers=headers, proxies=proxies_cfg, timeout=12)
        resp.raise_for_status()
        results_json = resp.json()
        data = results_json.get("data", [])
        if not data:
            return {"error": f"æœªæ‰¾åˆ°ç»“æœ: {query}", "sources": {}}
        sources = {}
        for i, item in enumerate(data[:max_sources], 1):
            title = (item.get("title") or "").strip()[:120]
            description = (item.get("description") or "").strip()[:MAX_SOURCE_DESC_LEN]
            url_link = (item.get("url") or "").strip()
            if description and url_link:
                sources[f"ä¿¡æ¯{i}"] = {"url": url_link, "description": description, "title": title}
        return {"sources": sources, "error": None} if sources else {"error": "ç»“æœæ— æœ‰æ•ˆå†…å®¹", "sources": {}}
    except Exception as e:
        return {"error": f"{type(e).__name__}: {e}", "sources": {}}


def extract_search_query(raw: str):
    match = re.search(r"<search>\s*([^<\n]+?)\s*</search>", raw, re.IGNORECASE)
    if match:
        query = match.group(1).strip()
        if query and len(query) <= 80:
            return query, raw[:match.end()]
    return None, None


def clean_final_response(text: str) -> str:
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r"<search>.*?</search>", "", text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r"</?(?:think|search)>", "", text, re.IGNORECASE)
    text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
    return text.strip()


def format_sources_for_prompt(sources_dict: dict, used_sources: dict) -> str:
    if not sources_dict:
        return "[æ— å¯ç”¨ä¿¡æ¯]"
    lines = ["**æœç´¢ç»“æœ:**"]
    for key, val in sources_dict.items():
        desc = val.get("description", "")[:300]
        url = val.get("url", "")
        title = val.get("title", "æœªå‘½å")
        lines.append(f"{key}({title}): {desc}")
        lines.append(f"  URL: {url}")
        if url and url not in used_sources:
            used_sources[url] = title
    return "\n".join(lines)


def run_search_agent(user_query, max_rounds=5, max_new_tokens=512, use_proxy=True):
    clear_model_cache()
    rounds = []
    prompt = f"{BASE_PROMPT}\nç”¨æˆ·é—®é¢˜: {user_query}"
    search_count = 0
    used_sources = {}

    for r in range(1, max_rounds + 1):
        _, raw, clean = generate(prompt, max_new_tokens=max_new_tokens)
        entry = {"round": r, "raw": raw, "clean": clean, "prompt": prompt}

        q, raw_trunc = extract_search_query(raw)
        if q:
            search_count += 1
            entry["raw"] = raw_trunc if raw_trunc else raw
            entry["search"] = q

            result_dict = fetch_search_result(q, use_proxy=use_proxy)
            entry["search_result"] = result_dict

            if result_dict.get("error"):
                print(f"âš ï¸ æœç´¢å¤±è´¥: {result_dict['error']}")
                rounds.append(entry)
                break

            sources = result_dict.get("sources", {})
            formatted_sources = format_sources_for_prompt(sources, used_sources)
            entry["formatted_sources"] = formatted_sources[:MAX_FORMATTED_SOURCES_LEN]
            rounds.append(entry)

            prompt = (
                f"{BASE_PROMPT}\n\n"
                f"=== ä»»åŠ¡å›é¡¾ ===\n"
                f"ç”¨æˆ·çš„åŸå§‹é—®é¢˜(query)æ˜¯: {user_query}\n\n"
                f"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹,è´Ÿè´£ä¸ºç”¨æˆ·æŸ¥è¯¢ä¿¡æ¯å¹¶ç»™å‡ºä¸“ä¸šç­”æ¡ˆã€‚\n"
                f"ä½ åœ¨ç¬¬{search_count}è½®æœç´¢ä¸­å·²ç»æŸ¥è¯¢äº†å…³é”®è¯'{q}',è·å¾—äº†ä»¥ä¸‹æœç´¢ç»“æœ:\n\n"
                f"{formatted_sources[:MAX_FORMATTED_SOURCES_LEN]}\n\n"
                f"=== å½“å‰ä»»åŠ¡(æå…¶é‡è¦!) ===\n"
                f"ç°åœ¨ä½ éœ€è¦åˆ†æè¿™äº›æœç´¢ç»“æœ,åˆ¤æ–­æ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜:\n\n"
                f"**æƒ…å†µ1: ä¿¡æ¯å·²ç»è¶³å¤Ÿ (è¿™æ˜¯æœ€æ ¸å¿ƒçš„ç¯èŠ‚!)**\n"
                f"å¦‚æœä»¥ä¸Šæœç´¢ç»“æœåŒ…å«äº†è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”'{user_query}',è¯·åŠ¡å¿…æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œ:\n\n"
                f"ç¬¬ä¸€æ­¥(æœ€é‡è¦!å¿…é¡»æ‰§è¡Œ!): ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜(query)\n"
                f"  - ç”¨1-3å¥è¯ç»™å‡ºæ ¸å¿ƒç­”æ¡ˆ,åŒ…å«å…·ä½“æ•°å­—/äº‹å®ï¼Œå›ç­”è¦å›´ç»•queryï¼Œç®€æ˜æ‰¼è¦\n"
                f"  - ä¾‹å¦‚ç”¨æˆ·é—®è‚¡ä»·,ä½ è¦è¯´'ç‰¹æ–¯æ‹‰(TSLA)å½“å‰è‚¡ä»·ä¸ºXXXç¾å…ƒ'\n"
                f"  - ä¸¥ç¦ç…§æŠ„è¾“å‡ºç¤ºä¾‹ï¼ä¸€å®šè¦æ ¹æ®å…ˆå‰çš„æ‰€æœ‰ä¿¡æ¯å’Œåˆ†æè‡ªå·±æ€»ç»“ï¼\n"
                f"  - ä¸è¦è·³è¿‡è¿™ä¸€æ­¥!è¿™æ˜¯ç”¨æˆ·æœ€éœ€è¦çš„!\n\n"
                f"ç¬¬äºŒæ­¥: è¡¥å……è¯¦ç»†è¯´æ˜(å¯é€‰)\n"
                f"  - å¦‚æœæœ‰é¢å¤–æœ‰ä»·å€¼çš„ä¿¡æ¯,ç®€è¦è¡¥å……\n"
                f"  - å¦‚æ¶¨è·Œå¹…ã€å¸‚å€¼ç­‰ç›¸å…³æ•°æ®\n\n"
                f"ç¬¬ä¸‰æ­¥: åˆ—å‡ºå‚è€ƒæ¥æº\n"
                f"  - æ ¼å¼:\n"
                f"    å‚è€ƒæ¥æº:\n"
                f"    [1] URL1\n"
                f"    [2] URL2\n\n"
                f"ç¬¬å››æ­¥: ä¸è¦å†è¾“å‡º<search>æ ‡ç­¾\n\n"
                f"è¾“å‡ºç¤ºä¾‹:\n"
                f"ç‰¹æ–¯æ‹‰(TSLA)å½“å‰è‚¡ä»·ä¸ºXXXç¾å…ƒ,è¾ƒå‰ä¸€äº¤æ˜“æ—¥ä¸Šæ¶¨XX%ã€‚\n\n"
                f"å‚è€ƒæ¥æº:\n"
                f"[1] https://...\n"
                f"[2] https://...\n\n"
                f"**æƒ…å†µ2: ä¿¡æ¯ä¸è¶³,éœ€è¦ç»§ç»­æœç´¢**\n"
                f"å¦‚æœæœç´¢ç»“æœä¸å¤Ÿè¯¦ç»†,æˆ–è€…ç¼ºå°‘å…³é”®ä¿¡æ¯:\n"
                f"1. åˆ†æç¼ºå°‘ä»€ä¹ˆä¿¡æ¯\n"
                f"2. è¾“å‡º<search>æ–°çš„æŸ¥è¯¢è¯</search>æ¥è·å–æ›´å¤šç»†èŠ‚\n"
                f"3. æ³¨æ„ä¸è¦é‡å¤æœç´¢ç›¸åŒçš„å…³é”®è¯\n\n"
                f"=== å…³é”®æé†’ ===\n"
                f"- è¿™äº›æœç´¢ç»“æœéƒ½æ˜¯ä½ è‡ªå·±æŸ¥è¯¢å¾—åˆ°çš„,ä¸æ˜¯ç”¨æˆ·æä¾›çš„\n"
                f"- ä½ å¿…é¡»ç»™å‡ºå®è´¨æ€§ç­”æ¡ˆ,ä¸èƒ½åªåˆ—å‡ºé“¾æ¥!ç”¨æˆ·éœ€è¦ä½ çš„æ€»ç»“!\n"
                f"- ä¿¡æ¯è¶³å¤Ÿå°±ç«‹å³å›ç­”,å…ˆç­”æ¡ˆåé“¾æ¥,ä¸è¦åªæœ‰é“¾æ¥æ²¡æœ‰ç­”æ¡ˆ!\n"
                f"- å›ç­”è¦ä¸“ä¸šç®€æ´,ç›´æ¥é’ˆå¯¹é—®é¢˜,ä¸è¦è¯´'æˆ‘éœ€è¦åˆ†æ'è¿™ç±»è¯\n\n"
                f"ç°åœ¨è¯·ç«‹å³ç»™å‡ºä½ çš„ç­”æ¡ˆ(è®°ä½:å…ˆå›ç­”é—®é¢˜,å†åˆ—å‚è€ƒæ¥æº):"
            )
            continue

        # æ—  <search> æ ‡ç­¾,è®¤ä¸ºæ˜¯æœ€ç»ˆç­”æ¡ˆ
        entry["clean"] = clean_final_response(clean)
        entry["used_sources"] = used_sources.copy()
        rounds.append(entry)
        break

    return rounds, used_sources


def show_rounds(rounds, used_sources=None, user_query=None):
    if user_query:
        print(f"Query: {user_query}")

    for i, info in enumerate(rounds, 1):
        print(f"\n{'='*80}")
        print(f"=== Round {info['round']} ===")

        print('Raw_responses:')
        raw = info.get('raw', '')
        print(raw[:MAX_RAW_DISPLAY_LEN] + '...' if len(raw) > MAX_RAW_DISPLAY_LEN else raw)

        if 'search' in info:
            print(f"\nğŸ” search_content: {info['search']}")

        if 'search_result' in info:
            result = info['search_result']
            if result.get('error'):
                print(f"\nâš ï¸ æœç´¢é”™è¯¯: {result['error']}")
            else:
                sources = result.get('sources', {})
                print(f"\nğŸ“š æœç´¢åˆ° {len(sources)} æ¡ä¿¡æ¯ï¼ˆä»…å±•ç¤ºå‰3æ¡ï¼‰:")
                for key, val in list(sources.items())[:3]:
                    print(f"  {key}: {val.get('description', '')[:100]}...")

        print()
        is_last = (i == len(rounds))
        if is_last:
            print('âœ… Clean_responses (æœ€ç»ˆç­”æ¡ˆ):')
            print(info.get('clean') or '[æ— æœ€ç»ˆå›ç­”]')
            if used_sources:
                print(f"\nğŸ“ æœ¬æ¬¡å¯¹è¯æ‰€æœ‰è¿½è¸ªçš„é“¾æ¥ (å…±{len(used_sources)}ä¸ª):")
                for idx, (url, title) in enumerate(list(used_sources.items())[:], 1):
                    print(f"  [{idx}] {title}\n      {url}")
        print('='*80)


# ============ CLI å…¥å£ ============

def main():
    parser = argparse.ArgumentParser(description="Search Agent CLI (æ ‡ç­¾æ£€æµ‹ + Jina æœç´¢)")
    parser.add_argument("-p", "--prompt", required=True, help="ç”¨æˆ·æŸ¥è¯¢è¯­å¥ï¼Œä¾‹å¦‚ï¼š-p æŸ¥è¯¢ç‰¹æ–¯æ‹‰çš„å®æ—¶è‚¡ä»·")
    parser.add_argument("--max-rounds", type=int, default=5, help="æœ€å¤§æœç´¢è½®æ¬¡ï¼Œé»˜è®¤ 5")
    parser.add_argument("--no-proxy", action="store_true", help="ä¸ä½¿ç”¨ä»£ç†è®¿é—® Jina")
    parser.add_argument("--max-new-tokens", type=int, default=512, help="å•è½®æœ€å¤§ç”Ÿæˆ token æ•°")
    args = parser.parse_args()

    user_query = args.prompt
    use_proxy = not args.no_proxy
    rounds, sources = run_search_agent(user_query, max_rounds=args.max_rounds, max_new_tokens=args.max_new_tokens, use_proxy=use_proxy)
    show_rounds(rounds, sources, user_query)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(130)
